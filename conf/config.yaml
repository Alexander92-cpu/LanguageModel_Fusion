root_params:
  seed: 42
  log_dir: ???
  device: cuda
  data_dir: data
  eval_do_lowercase: true
  data_url: https://huggingface.co/AlexanderMaz/LanguageModel_Fusion

lm:
  dir_data: data/text/librispeech
  block_size: 512

tokenizer:
  start_token: '‚ñÅ'
  blank_idx: 1024

kenlm:
  ngram: 4
  model: ${root_params.data_dir}/kenlm/${kenlm.ngram}_ngram_output.bin
  train_file: ${root_params.data_dir}/text/ngram/train.txt
  kenlm_bin_path: kenlm/build/bin
  do_lowercase: true
  remove_temp_files: false
  offset: 100

gpt2:
  model_name: gpt2-medium
  dir_model: ${root_params.data_dir}/gpt2
  output_dir: ${root_params.data_dir}/gpt2_train
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 2
  evaluation_strategy: epoch
  save_strategy: epoch
  save_total_limit: 4
  logging_steps: 100
  gradient_accumulation_steps: 1
  num_train_epochs: 10
  weight_decay: 0.0
  warmup_steps: 1000
  lr_scheduler_type: cosine
  learning_rate: 5e-4
  fp16: True
  logging_dir: ${root_params.log_dir}
  load_best_model_at_end: True
  report_to: tensorboard
  seed: ${root_params.seed}

lstm:
  dir_lstm_data: ${root_params.data_dir}/text/lstm
  save: ${root_params.data_dir}/lstm/model.pt
  tokenizer_path: ${root_params.data_dir}/lstm/tokenizer.pkl
  model_type: LSTM
  emsize: 512
  nhid: 512
  nlayers: 6
  dropout: 0.2
  tied: True
  words_limit: 70000
  num_words: 70003
  cuda: true
  device: ${root_params.device}
  mps: false
  lr: 20
  epochs: 40
  batch_size: 20
  eval_batch_size: 10
  bptt: 35
  seed: ${root_params.seed}
  log_interval: 200
  onnx_export: ''
  nhead: 2
  dry_run: false
  clip: 0.25
  temperature: 1.0

asr_model:
  model: ${root_params.data_dir}/asr/stt_en_conformer_transducer_small.nemo
  strategy: beam
  beam_size: 16
  batch_size: 1

rescore:
  calculate_wer: true
  methods:
    baseline: true
    lodr: true
    dr: true
    ilme: true
    sf: true
  params:
    baseline: {}
    sf:
      gpt2_scores: 0.2
      num_tokens: 0.1
    dr:
      gpt2_scores: 4.2
      dr_scores: -1.2
      num_tokens: 7.6
    lodr:
      gpt2_scores: 6.7
      lodr_scores: -0.2
      num_tokens: 9.1
    ilme:
      gpt2_scores: 1.4
      ilme_scores: -0.09
      num_tokens: 1.8

optimize:
  db_exp: data/optimize/exp.db
  optimize_data_file: data/optimize/data_all.pkl
  load_if_exists: true
  n_trials: 2000
  n_jobs: 1
  step: 0.1
  bounds:
      sf:
        gpt2_scores: [0, 10]
        num_tokens: [0, 10]
      dr:
        gpt2_scores: [0, 10]
        dr_scores: [-10, 0]
        num_tokens: [0, 10]
      lodr:
        gpt2_scores: [0, 10]
        lodr_scores: [-10, 0]
        num_tokens: [0, 10]
      ilme:
        gpt2_scores: [0, 10]
        ilme_scores: [-10, 0]
        num_tokens: [0, 10]
